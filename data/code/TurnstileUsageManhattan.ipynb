{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging subway clean (from Google Sheet - https://goo.gl/2WQxkV) to turnstile data\n",
    "subway_clean_fp = 'subway_clean_raw.csv'\n",
    "subway_clean = pd.read_csv(subway_clean_fp)\n",
    "subway_clean = subway_clean.drop(columns=['Location','Comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting clean subway coord data, keeping only single station name\n",
    "subway_clean.sort_values(['ID','Station'],inplace=True)\n",
    "\n",
    "master_names = subway_clean.groupby('ID').agg({'Station':lambda x: x.iloc[0],\n",
    "                               'Line Name':lambda x: x.iloc[0],\n",
    "                                'Division':lambda x: x.iloc[0],\n",
    "                                'Latitude':lambda x: x.iloc[0],\n",
    "                                'Longitude':lambda x: x.iloc[0]\n",
    "                               })\n",
    "# Add back in line 1 to Chamber St, Change name\n",
    "master_names.loc[139,'Line Name'] = 'ACE123'\n",
    "master_names.loc[146,'Station'] = 'DELANCEY/ESSEX ST'\n",
    "master_names.loc[20,'Station'] = '14 ST/8 AV'\n",
    "# # Export this if needed\n",
    "# master_names.to_csv('subway_coords_clean.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From NY State Open Data\n",
    "turnstile_fp = 'Turnstile_Usage_Data__2015.csv'\n",
    "turnstile = pd.read_csv(turnstile_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sc = turnstile.merge(subway_clean,how='inner',left_on=['Station','Line Name','Division'],\n",
    "                       right_on=['Station','Line Name','Division'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Somehow the above merge results in duplicate traffic data\n",
    "dedupe = t_sc.drop_duplicates(subset=['Unit','SCP','Date','Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dwoo/virtualenvs/dsi/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Generate date_time column\n",
    "space = np.full(dedupe.shape[0],' ',dtype=str)\n",
    "dedupe['date_time'] = pd.to_datetime(dedupe.Date + space + dedupe.Time,format=\"%m/%d/%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No longer necessary\n",
    "dedupe = dedupe.drop(columns='Date')\n",
    "# Bizzare column name cleanup\n",
    "dedupe.rename(columns={'Exits                                                     ':'Exits'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort values first\n",
    "dedupe.sort_values(['Unit','SCP','Station','Line Name',\n",
    "                   'ID','Latitude','Longitude','date_time'],inplace=True)\n",
    "timed = dedupe.groupby(['Unit','SCP','Station','Line Name','ID','Latitude','Longitude',\n",
    "                pd.Grouper(key='date_time',freq='4h',closed='left',base=0)])\\\n",
    "                .agg({'Entries':lambda x: x.iloc[0],\n",
    "                      'Exits':lambda x: x.iloc[0]}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "timed['date_time_next'] = timed.date_time + pd.Timedelta('4h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging by offset time column to calculate delta\n",
    "timed = timed.merge(timed,how='inner',left_on=['Unit','SCP','Station','Line Name','ID','Latitude','Longitude','date_time'],\n",
    "            right_on=['Unit','SCP','Station','Line Name','ID','Latitude','Longitude','date_time_next'],\n",
    "            suffixes=('','_prior'))\n",
    "\n",
    "timed['entry_volume'] = timed.Entries - timed.Entries_prior\n",
    "timed['exit_volume'] = timed.Exits - timed.Exits_prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Quick note on issue with data**\n",
    "\n",
    "There are negative entry/exit volumes, as well as exorbitantly high entry/exit volumes as well\n",
    "\n",
    "The negative values seem to come from a couple possibilities:\n",
    "1. Some turnstiles record cumulative entry/exits in seemingly reverse chronological order (see example below)\n",
    "`dedupe[(dedupe.Unit == 'R011') & (dedupe.SCP == '00-00-04') & (dedupe.Date == '03/05/2015') & (dedupe.Station == '42 ST-PA BUS TE') & (dedupe['Line Name'] == 'ACENQRS1237')]`\n",
    "\n",
    "2. Some turnstiles seem to reset their counters, where the cumulative record sees a steep drop all of a sudden\n",
    "`dedupe[(dedupe.Unit == 'R548') & (dedupe.SCP == '00-00-01') & (dedupe.Date == '07/22/2015') & (dedupe.Station == 'CHRISTOPHER ST') & (dedupe['Line Name'] == '1')]`\n",
    "\n",
    "To deal with this, I've done the following:\n",
    "- Convert all negative entry/exit volumes into positive values (assume the reverse records are mistakes)\n",
    "- Drop all values that are above the 0.9999 quantile (which is around 3000/4hr/device, or around 13/min/device), by replacing them with np.NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "timed['entry_volume'] = timed.entry_volume.abs()\n",
    "timed['exit_volume'] = timed.exit_volume.abs()\n",
    "\n",
    "extreme_entry = timed.entry_volume.quantile(0.9999)\n",
    "extreme_exit = timed.exit_volume.quantile(0.9999)\n",
    "\n",
    "timed['entry_volume'] = np.where(timed.entry_volume > extreme_entry,np.NaN,timed.entry_volume)\n",
    "timed['exit_volume'] = np.where(timed.exit_volume > extreme_exit,np.NaN,timed.exit_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using same logic for cleaning up subway coordinates file to keep 1 row per station/time\n",
    "consolidated = timed.groupby(['ID','date_time','date_time_prior']).agg(\n",
    "                            {'entry_volume':sum,\n",
    "                            'exit_volume':sum\n",
    "                            }).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add station attributes from master dataset of names\n",
    "consolidated['station'] = consolidated.ID.map(master_names['Station'])\n",
    "consolidated['lines'] = consolidated.ID.map(master_names['Line Name'])\n",
    "consolidated['division'] = consolidated.ID.map(master_names['Division'])\n",
    "consolidated['latitude'] = consolidated.ID.map(master_names['Latitude'])\n",
    "consolidated['longitude'] = consolidated.ID.map(master_names['Longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding some time attributes\n",
    "consolidated['day_of_week'] = consolidated.date_time.dt.dayofweek\n",
    "consolidated['day'] = consolidated.date_time.dt.weekday_name\n",
    "consolidated['hour'] = consolidated.date_time.dt.hour\n",
    "consolidated['date'] = consolidated.date_time.dt.date\n",
    "consolidated['interval'] = consolidated.date_time_prior.dt.strftime(date_format=\"%I%p\")+\\\n",
    "                            np.full(consolidated.shape[0],'-')+\\\n",
    "                            consolidated.date_time.dt.strftime(date_format=\"%I%p\")\n",
    "us_holidays = holidays.UnitedStates()\n",
    "consolidated['is_holiday'] = consolidated.date_time.dt.date.apply(lambda x: x in us_holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reordered_columns = ['date','date_time','interval','day_of_week','day','hour','is_holiday',\n",
    "                     'ID','station','lines','entry_volume','exit_volume']\n",
    "master_volume = consolidated[reordered_columns].rename(columns={'ID':'station_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_volume.to_csv('2015_manhattan_turnstile_usage.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
